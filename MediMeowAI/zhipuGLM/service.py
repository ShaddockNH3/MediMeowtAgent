import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

import os
import json
from typing import List, Generator, Union, Optional
from operator import itemgetter

from zhipuai import ZhipuAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma

# å¯¼å…¥åº•å±‚ä¾èµ–æ¨¡å—
import config.config as config
import prompts.prompts as prompts
import utils.utils as utils
import rag.rag_core as rag_core

# -----------------------------------------------------------------
# 1. Protobuf æ¶ˆæ¯ç»“æ„æ¨¡æ‹Ÿ
# -----------------------------------------------------------------

class AnalysisRequest:
    """æ¨¡æ‹Ÿ Protobuf è¾“å…¥æ¶ˆæ¯ç»“æ„ã€‚"""
    def __init__(self, patient_text_data: str, image_base64: str, stream: bool = False):
        self.patient_text_data = patient_text_data
        self.image_base64 = image_base64
        self.stream = stream

class AnalysisReport:
    """æ¨¡æ‹Ÿ Protobuf è¾“å‡ºæ¶ˆæ¯ç»“æ„"""
    def __init__(self, structured_report: str, status: str = "SUCCESS"):
        self.structured_report = structured_report
        self.status = status

# æµå¼ä¼ è¾“çš„è¾“å‡ºç±»å‹
StreamReport = Generator[str, None, None]

# -----------------------------------------------------------------
# 2. å…¨å±€ä¾èµ– 
# -----------------------------------------------------------------
GLOBAL_VECTOR_STORE: Optional[Chroma] = None
GLOBAL_LLM = None
GLOBAL_ZHIPU_CLIENT: Optional[ZhipuAI] = None

def initialize_service():
    """
    æœåŠ¡åˆå§‹åŒ–å‡½æ•°ï¼šåŠ è½½ LLM å®ä¾‹ã€å‘é‡æ•°æ®åº“å’Œæ™ºè°±å®¢æˆ·ç«¯ã€‚
    æ­¤å‡½æ•°å¿…é¡»åœ¨æœåŠ¡ï¼ˆå¦‚ FastAPI åº”ç”¨ï¼‰å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ã€‚
    """
    global GLOBAL_VECTOR_STORE
    global GLOBAL_LLM
    global GLOBAL_ZHIPU_CLIENT
    
    try:
        GLOBAL_VECTOR_STORE = rag_core.build_or_load_rag_index()
        GLOBAL_LLM = utils.get_glm4_llm()
        GLOBAL_ZHIPU_CLIENT = ZhipuAI(api_key=os.environ["GLM_API_KEY"])
    except Exception as e:
        print(f"æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        GLOBAL_VECTOR_STORE = None
        GLOBAL_LLM = None
        GLOBAL_ZHIPU_CLIENT = None
        
def _stage1_generate_description(llm, patient_text_data: str, image_base64: str) -> str:
    messages_stage1 = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å®¢è§‚çš„åŒ»ç–—åŠ©æ‰‹ï¼Œä¸¥æ ¼æŒ‰ç…§æä¾›çš„æ ¼å¼è¾“å‡ºã€‚"),
        HumanMessage(
            content=[
                {"type": "text", "text": prompts.STAGE1_PROMPT_TEMPLATE.format(text_input=patient_text_data)},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
            ]
        )
    ]
    response = llm.invoke(messages_stage1)
    return response.content

def _stage2_retrieve_context(llm, multimodal_description_block: str, vector_store: Chroma) -> str:
    keyword_prompt = ChatPromptTemplate.from_template(prompts.RAG_RETRIEVAL_PROMPT)
    keyword_chain = keyword_prompt | llm | (lambda x: x.content)
    retrieval_keywords = keyword_chain.invoke({"report_fragment": multimodal_description_block})

    retriever = vector_store.as_retriever(search_kwargs={"k": 5})
    retrieved_docs: List[Document] = retriever.invoke(retrieval_keywords) 
    retrieved_context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
    return retrieved_context

def _stage3_sync_generate_final_report(llm, patient_text_data: str, multimodal_description_block: str, retrieved_context: str) -> str:
    final_prompt = ChatPromptTemplate.from_template(prompts.FINAL_REPORT_PROMPT)
    final_chain = final_prompt | llm | (lambda x: x.content)
    final_report = final_chain.invoke({
        "original_text_data": patient_text_data,
        "multimodal_description": multimodal_description_block, 
        "retrieved_context": retrieved_context
    })
    return final_report

def _stage3_stream_generate_final_report(client: ZhipuAI, patient_text_data: str, multimodal_description_block: str, retrieved_context: str) -> StreamReport:
    prompt_text = prompts.FINAL_REPORT_PROMPT.format(
        original_text_data=patient_text_data,
        multimodal_description=multimodal_description_block,
        retrieved_context=retrieved_context
    )
    
    response = client.chat.completions.create(
        model="glm-4.1v-thinking-flash",
        messages=[{"role": "user", "content": prompt_text}],
        temperature=config.TEMPERATURE,
        max_tokens=config.MAX_TOKENS,
        stream=True
    )
    
    for chunk in response:
        if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content
        
        if chunk.choices and chunk.choices[0].finish_reason:
            yield "[STREAM_END]"


# -----------------------------------------------------------------
# 3. æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ 
# -----------------------------------------------------------------

def process_medical_analysis(request: AnalysisRequest) -> Union[AnalysisReport, StreamReport]:
    """
    æ ¸å¿ƒåˆ†æå‡½æ•°ï¼šå…¼å®¹åŒæ­¥å’Œæµå¼ä¼ è¾“ã€‚
    
    Args:
        request: åŒ…å«åŸå§‹æ–‡æœ¬å’Œå›¾ç‰‡Base64ç¼–ç çš„è¯·æ±‚å¯¹è±¡ã€‚
        
    Returns:
        AnalysisReport (åŒæ­¥æ¨¡å¼) æˆ– Generator[str] (æµå¼æ¨¡å¼)ã€‚
    """
    
    if GLOBAL_VECTOR_STORE is None or GLOBAL_LLM is None or GLOBAL_ZHIPU_CLIENT is None:
        return AnalysisReport(structured_report="åŒ»ç–—åˆ†ææœåŠ¡æœªå°±ç»ªï¼Œè¯·æ£€æŸ¥åˆå§‹åŒ–çŠ¶æ€ã€‚", status="SERVICE_UNAVAILABLE")

    try:
        # é˜¶æ®µ 1 å’Œ 2 å¿…é¡»åŒæ­¥å®Œæˆ
        multimodal_description_block = _stage1_generate_description(GLOBAL_LLM, request.patient_text_data, request.image_base64)
        retrieved_context = _stage2_retrieve_context(GLOBAL_LLM, multimodal_description_block, GLOBAL_VECTOR_STORE)
        
        # é˜¶æ®µ 3: æ ¹æ®æ¨¡å¼é€‰æ‹©åŒæ­¥æˆ–æµå¼ç”Ÿæˆ
        if request.stream:
            return _stage3_stream_generate_final_report(
                GLOBAL_ZHIPU_CLIENT, 
                request.patient_text_data, 
                multimodal_description_block, 
                retrieved_context
            )
        else:
            final_report_text = _stage3_sync_generate_final_report(
                GLOBAL_LLM, 
                request.patient_text_data, 
                multimodal_description_block, 
                retrieved_context
            )
            return AnalysisReport(structured_report=final_report_text, status="SUCCESS")
        
    except Exception as e:
        error_msg = f"ç³»ç»Ÿå†…éƒ¨é”™è¯¯ï¼Œæ— æ³•å®Œæˆåˆ†æã€‚è¯¦æƒ…: {type(e).__name__}: {str(e)}"
        print(f"âŒ AIåˆ†æè¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {error_msg}")
        import traceback
        print(f"ğŸ” è¯¦ç»†å †æ ˆ:\n{traceback.format_exc()}")
        return AnalysisReport(structured_report=error_msg, status="INTERNAL_ERROR")
